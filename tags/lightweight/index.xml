<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LightWeight on Xiiin&#39;s Blog</title>
    <link>https://wxding0324.github.io/tags/lightweight/</link>
    <description>Recent content in LightWeight on Xiiin&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© 2025 Xiiin</copyright>
    <lastBuildDate>Tue, 12 Nov 2024 16:34:19 +0800</lastBuildDate><atom:link href="https://wxding0324.github.io/tags/lightweight/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>小模型的搭建和训练tricks</title>
      <link>https://wxding0324.github.io/python/tricks/%E5%B0%8F%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5/lw/</link>
      <pubDate>Tue, 12 Nov 2024 16:34:19 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/python/tricks/%E5%B0%8F%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5/lw/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature  Refinement for Image Restoration</title>
      <link>https://wxding0324.github.io/paperreading/cvpr/ast/</link>
      <pubDate>Mon, 04 Nov 2024 09:37:14 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/cvpr/ast/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>Drop an Octave: Reducing Spatial Redundancy in  Convolutional Neural Networks with Octave Convolution</title>
      <link>https://wxding0324.github.io/paperreading/iccv/octave/</link>
      <pubDate>Sun, 20 Oct 2024 09:37:14 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/iccv/octave/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks</title>
      <link>https://wxding0324.github.io/paperreading/cvpr/fasternet/</link>
      <pubDate>Fri, 18 Oct 2024 12:29:30 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/cvpr/fasternet/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>Split to Be Slim: An Overlooked Redundancy in Vanilla Convolution</title>
      <link>https://wxding0324.github.io/paperreading/ijcai/spconv/</link>
      <pubDate>Wed, 16 Oct 2024 16:09:14 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/ijcai/spconv/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>GhostNetV2: Enhance Cheap Operation with Long-Range Attention</title>
      <link>https://wxding0324.github.io/paperreading/nips/ghostnetv2/</link>
      <pubDate>Tue, 15 Oct 2024 16:24:19 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/nips/ghostnetv2/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>GhostNet: More Features from Cheap Operations</title>
      <link>https://wxding0324.github.io/paperreading/cvpr/ghostnet/</link>
      <pubDate>Tue, 15 Oct 2024 16:15:36 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/cvpr/ghostnet/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>NOT ALL PATCHES ARE WHAT YOU NEED: EXPEDITING VISION TRANSFORMERS VIA TOKEN REORGANIZATIONS</title>
      <link>https://wxding0324.github.io/paperreading/iclr/evit/</link>
      <pubDate>Tue, 23 Jul 2024 10:32:27 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/iclr/evit/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</title>
      <link>https://wxding0324.github.io/paperreading/nips/dynamicvit/</link>
      <pubDate>Tue, 23 Jul 2024 10:24:29 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/nips/dynamicvit/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>SHaRPose: Sparse High-Resolution Representation for Human Pose Estimation</title>
      <link>https://wxding0324.github.io/paperreading/aaai/sharpose/</link>
      <pubDate>Sun, 07 Jul 2024 22:01:01 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/aaai/sharpose/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition</title>
      <link>https://wxding0324.github.io/paperreading/aaai/lf-vit/</link>
      <pubDate>Sun, 07 Jul 2024 21:45:54 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/aaai/lf-vit/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>CF-ViT: A General Coarse-to-Fine Method for Vision Transformer</title>
      <link>https://wxding0324.github.io/paperreading/aaai/cf-vit/</link>
      <pubDate>Sun, 07 Jul 2024 17:41:32 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/aaai/cf-vit/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>DTMFormer: Dynamic Token Merging for Boosting Transformer-Based Medical Image Segmentationer</title>
      <link>https://wxding0324.github.io/paperreading/aaai/dtmformer/</link>
      <pubDate>Sat, 08 Jun 2024 20:17:07 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/aaai/dtmformer/</guid>
      <description></description>
      
    </item>
    
    <item>
      <title>BiFormer: Vision Transformer with Bi-Level Routing Attention</title>
      <link>https://wxding0324.github.io/paperreading/cvpr/biformer/</link>
      <pubDate>Wed, 08 May 2024 11:25:52 +0800</pubDate>
      
      <guid>https://wxding0324.github.io/paperreading/cvpr/biformer/</guid>
      <description></description>
      
    </item>
    
  </channel>
</rss>
